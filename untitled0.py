# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1myKtNY6dPAu5yQ8tFSMgUDhjLlCQ3C-2
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.fft import fft
import re

# --- 1. Cargar texto del discurso (puedes pegar directamente desde una fuente citada) ---
speech = """Chief Justice Roberts, President Carter, President Clinton, President Bush, President Obama, fellow Americans, and people of the world: thank you.

We, the citizens of America, are now joined in a great national effort to rebuild our country and to restore its promise for all of our people.

Together, we will determine the course of America and the world for years to come.

We will face challenges. We will confront hardships. But we will get the job done.

Every four years, we gather on these steps to carry out the orderly and peaceful transfer of power, and we are grateful to President Obama and First Lady Michelle Obama for their gracious aid throughout this transition. They have been magnificent.

Today’s ceremony, however, has very special meaning. Because today we are not merely transferring power from one Administration to another, or from one party to another – but we are transferring power from Washington, D.C. and giving it back to you, the American People.

For too long, a small group in our nation’s Capital has reaped the rewards of government while the people have borne the cost.

Washington flourished – but the people did not share in its wealth.

Politicians prospered – but the jobs left, and the factories closed.

The establishment protected itself, but not the citizens of our country.

Their victories have not been your victories; their triumphs have not been your triumphs; and while they celebrated in our nation’s Capital, there was little to celebrate for struggling families all across our land.

That all changes – starting right here, and right now, because this moment is your moment: it belongs to you.

It belongs to everyone gathered here today and everyone watching all across America.

This is your day. This is your celebration.

And this, the United States of America, is your country.

What truly matters is not which party controls our government, but whether our government is controlled by the people.

January 20th 2017, will be remembered as the day the people became the rulers of this nation again.

The forgotten men and women of our country will be forgotten no longer.

Everyone is listening to you now.

You came by the tens of millions to become part of a historic movement the likes of which the world has never seen before.

At the center of this movement is a crucial conviction: that a nation exists to serve its citizens.

Americans want great schools for their children, safe neighborhoods for their families, and good jobs for themselves.

These are the just and reasonable demands of a righteous public.

But for too many of our citizens, a different reality exists: Mothers and children trapped in poverty in our inner cities; rusted-out factories scattered like tombstones across the landscape of our nation; an education system, flush with cash, but which leaves our young and beautiful students deprived of knowledge; and the crime and gangs and drugs that have stolen too many lives and robbed our country of so much unrealized potential.

This American carnage stops right here and stops right now.

We are one nation – and their pain is our pain. Their dreams are our dreams; and their success will be our success. We share one heart, one home, and one glorious destiny.

The oath of office I take today is an oath of allegiance to all Americans.

For many decades, we’ve enriched foreign industry at the expense of American industry;

Subsidized the armies of other countries while allowing for the very sad depletion of our military;

We’ve defended other nation’s borders while refusing to defend our own;

And spent trillions of dollars overseas while America’s infrastructure has fallen into disrepair and decay.

We’ve made other countries rich while the wealth, strength, and confidence of our country has disappeared over the horizon.

One by one, the factories shuttered and left our shores, with not even a thought about the millions upon millions of American workers left behind.

The wealth of our middle class has been ripped from their homes and then redistributed across the entire world.

But that is the past. And now we are looking only to the future.

We assembled here today are issuing a new decree to be heard in every city, in every foreign capital, and in every hall of power.

From this day forward, a new vision will govern our land.

From this moment on, it’s going to be America First.

Every decision on trade, on taxes, on immigration, on foreign affairs, will be made to benefit American workers and American families.

We must protect our borders from the ravages of other countries making our products, stealing our companies, and destroying our jobs. Protection will lead to great prosperity and strength.

I will fight for you with every breath in my body – and I will never, ever let you down.

America will start winning again, winning like never before.

We will bring back our jobs. We will bring back our borders. We will bring back our wealth. And we will bring back our dreams.

We will build new roads, and highways, and bridges, and airports, and tunnels, and railways all across our wonderful nation.

We will get our people off of welfare and back to work – rebuilding our country with American hands and American labor.

We will follow two simple rules: Buy American and Hire American.

We will seek friendship and goodwill with the nations of the world – but we do so with the understanding that it is the right of all nations to put their own interests first.

We do not seek to impose our way of life on anyone, but rather to let it shine as an example for everyone to follow.

We will reinforce old alliances and form new ones – and unite the civilized world against Radical Islamic Terrorism, which we will eradicate completely from the face of the Earth.

At the bedrock of our politics will be a total allegiance to the United States of America, and through our loyalty to our country, we will rediscover our loyalty to each other.

When you open your heart to patriotism, there is no room for prejudice.

The Bible tells us, “how good and pleasant it is when God’s people live together in unity.”

We must speak our minds openly, debate our disagreements honestly, but always pursue solidarity.

When America is united, America is totally unstoppable.

There should be no fear – we are protected, and we will always be protected.

We will be protected by the great men and women of our military and law enforcement and, most importantly, we are protected by God.

Finally, we must think big and dream even bigger.

In America, we understand that a nation is only living as long as it is striving.

We will no longer accept politicians who are all talk and no action – constantly complaining but never doing anything about it.

The time for empty talk is over.

Now arrives the hour of action.

Do not let anyone tell you it cannot be done. No challenge can match the heart and fight and spirit of America.

We will not fail. Our country will thrive and prosper again.

We stand at the birth of a new millennium, ready to unlock the mysteries of space, to free the Earth from the miseries of disease, and to harness the energies, industries and technologies of tomorrow.

A new national pride will stir our souls, lift our sights, and heal our divisions.

It is time to remember that old wisdom our soldiers will never forget: that whether we are black or brown or white, we all bleed the same red blood of patriots, we all enjoy the same glorious freedoms, and we all salute the same great American Flag.

And whether a child is born in the urban sprawl of Detroit or the windswept plains of Nebraska, they look up at the same night sky, they fill their heart with the same dreams, and they are infused with the breath of life by the same almighty Creator.

So to all Americans, in every city near and far, small and large, from mountain to mountain, and from ocean to ocean, hear these words:

You will never be ignored again.

Your voice, your hopes, and your dreams, will define our American destiny. And your courage and goodness and love will forever guide us along the way.

Together, We Will Make America Strong Again.

We Will Make America Wealthy Again.

We Will Make America Proud Again.

We Will Make America Safe Again.

And, Yes, Together, We Will Make America Great Again. Thank you, God Bless You, And God Bless America.
...
Together, We Will Make America Strong Again. We Will Make America Wealthy Again. We Will Make America Proud Again. We Will Make America Safe Again. And Yes, together, We Will Make America Great Again.
Thank you, God Bless America.
"""

# --- 2. Preprocesar el texto ---
text_clean = re.sub(r'[^\w\s]', '', speech.lower())
words = text_clean.split()

# --- 3. Señal: bloque de palabras ----------
block_size = 10
blocks = [words[i:i+block_size] for i in range(0, len(words), block_size)]

# --- 4. Analizar varias palabras objetivo ----
targets = ["America", "Will", "People", "We", "together"]
signals = {t: [b.count(t) for b in blocks] for t in targets}

# --- 5. FFT y gráficas para cada palabra ---
fig, axes = plt.subplots(len(targets), 2, figsize=(12, 4*len(targets)))

for row, target in enumerate(targets):
    freq_series = signals[target]
    fft_mag = np.abs(fft(freq_series))
    ax0, ax1 = axes[row]
    ax0.stem(freq_series)
    ax0.set_title(f"Frecuencia de '{target}' por bloque")
    ax0.set_xlabel("Bloque"); ax0.set_ylabel("Cuenta")

    ax1.stem(fft_mag)
    ax1.set_title(f"Espectro FFT de '{target}'")
    ax1.set_xlabel("Frecuencia"); ax1.set_ylabel("Magnitud")

plt.tight_layout()
plt.show()



"""# Task
Explain the error in the selected code, fix it if possible, and incorporate the changes into the existing code. Otherwise, diagnose the error.

## Analyze fft results

### Subtask:
Examine the generated FFT plots for each target word to identify dominant frequencies and discuss their potential meaning in the context of the speech.

**Reasoning**:
Examine the generated plots and analyze the FFT spectrum to identify dominant frequencies and discuss their potential meaning in the context of the speech.
"""

# Review the generated plots (visual inspection of the output from the previous cell)
# Identify dominant frequencies (peaks) in each FFT plot, excluding the DC component (frequency 0)
# Discuss what these frequencies might indicate about the usage pattern of the corresponding word

print("Analysis of FFT Plots:")
print("----------------------")

for target in targets:
    freq_series = signals[target]
    fft_mag = np.abs(fft(freq_series))
    # Exclude DC component for finding dominant non-zero frequencies
    fft_mag_nonzero = fft_mag[1:]

    if len(fft_mag_nonzero) > 0:
        dominant_freq_index = np.argmax(fft_mag_nonzero) + 1 # Add 1 to get the correct index in the original FFT array
        dominant_freq_magnitude = fft_mag[dominant_freq_index]
        # The actual frequency value is (dominant_freq_index / len(freq_series)) * sampling_frequency
        # Here, the 'sampling frequency' is 1 block per block, so it's 1.
        # The frequency represents cycles per block sequence.
        # For simplicity, we can just refer to the index as a relative frequency.
        relative_dominant_freq = dominant_freq_index / len(freq_series)

        print(f"Word: '{target}'")
        print(f"  Length of signal (number of blocks): {len(freq_series)}")
        print(f"  Dominant non-zero frequency index: {dominant_freq_index}")
        print(f"  Magnitude at dominant non-zero frequency: {dominant_freq_magnitude:.2f}")
        print(f"  Relative dominant frequency (cycles per sequence): {relative_dominant_freq:.4f}")

        # Discuss potential meaning
        if relative_dominant_freq < 0.1:
            print("  Interpretation: Low dominant frequency suggests the word appears somewhat consistently or in relatively large clusters across the speech.")
        elif relative_dominant_freq < 0.3:
             print("  Interpretation: Medium-low dominant frequency suggests the word appears with some regularity or in moderate clusters.")
        else:
            print("  Interpretation: Higher dominant frequency suggests the word appears more sporadically or in smaller, more dispersed clusters.")

        # Relate to original frequency plot (visual comparison)
        print(f"  Compare this to the '{target}' frequency plot to see if the pattern aligns.")
    else:
        print(f"Word: '{target}' - No non-zero frequencies to analyze.")

print("\nLimitations of this analysis:")
print("-----------------------------")
print("- The block size (10 words) is arbitrary and can significantly impact the frequency analysis.")
print("- FFT is best suited for periodic signals. Word usage in a speech is often not strictly periodic.")
print("- Dominant frequencies indicate patterns, but don't explain *why* those patterns exist (e.g., rhetorical structure, specific themes).")
print("- The interpretation of 'dominant frequency' is simplified here; more complex spectral analysis could reveal richer patterns.")

"""## Analyze other words

### Subtask:
Select additional words from the speech to analyze their frequency distribution and FFT spectrum.

**Reasoning**:
I need to select additional words from the speech and add them to the targets list to analyze their frequency and FFT spectrum. I will then re-run the code to generate the plots for all target words, including the new ones.
"""

# Identify additional potentially interesting words
# Based on reading, words like "country", "great", "nation", "jobs" seem relevant to the themes.
additional_targets = ["country", "great", "nation", "jobs"]

# Add these words to the existing targets list
targets.extend(additional_targets)

# Re-calculate signals for all targets
signals = {t: [b.count(t) for b in blocks] for t in targets}

# Re-generate FFT and plots for all words in the updated targets list
fig, axes = plt.subplots(len(targets), 2, figsize=(12, 4*len(targets)))

for row, target in enumerate(targets):
    freq_series = signals[target]
    fft_mag = np.abs(fft(freq_series))
    ax0, ax1 = axes[row]
    ax0.stem(freq_series)
    ax0.set_title(f"Frecuencia de '{target}' por bloque")
    ax0.set_xlabel("Bloque"); ax0.set_ylabel("Cuenta")

    ax1.stem(fft_mag)
    ax1.set_title(f"Espectro FFT de '{target}'")
    ax1.set_xlabel("Frecuencia"); ax1.set_ylabel("Magnitud")

plt.tight_layout()
plt.show()

"""## Alternative visualization

### Subtask:
Create a different visualization of the word frequencies, such as a bar chart or a heatmap.

**Reasoning**:
The subtask requires creating a heatmap visualization of word frequencies across blocks. This involves creating a pandas DataFrame from the signals dictionary and then using seaborn to generate the heatmap.
"""

import pandas as pd
import seaborn as sns

# Create a DataFrame from the signals dictionary
# The keys become columns (words), the lists become rows (blocks)
df_signals = pd.DataFrame(signals)

# Generate the heatmap
plt.figure(figsize=(10, len(targets)*0.8)) # Adjust figure size based on number of words
sns.heatmap(df_signals.T, cmap="YlGnBu", annot=False, fmt=".0f", linewidths=.5)

# Set labels and title
plt.xlabel("Block Number")
plt.ylabel("Words")
plt.title("Word Frequency Heatmap Across Speech Blocks")

# Display the plot
plt.show()

"""## Other text analysis

### Subtask:
Perform another type of text analysis, such as sentiment analysis or topic modeling, on the speech.

**Reasoning**:
I will use sentiment analysis on the cleaned text to understand the overall emotional tone of the speech. I will use the NLTK library for this purpose as it is a common and straightforward choice for sentiment analysis in Python.
"""

import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Download VADER lexicon if not already present
try:
    nltk.data.find('sentiment/vader_lexicon.zip')
except nltk.downloader.DownloadError:
    nltk.download('vader_lexicon')

# Initialize VADER sentiment intensity analyzer
analyzer = SentimentIntensityAnalyzer()

# Get sentiment scores for the cleaned speech text
sentiment_scores = analyzer.polarity_scores(text_clean)

# Print the sentiment scores
print("Sentiment Analysis Results:")
print(sentiment_scores)

# Interpret the results (optional, based on scores)
if sentiment_scores['compound'] > 0.05:
    print("Overall Sentiment: Positive")
elif sentiment_scores['compound'] < -0.05:
    print("Overall Sentiment: Negative")
else:
    print("Overall Sentiment: Neutral")

"""**Reasoning**:
The previous code failed because `nltk.downloader.DownloadError` does not exist. The traceback suggests using `nltk.download('vader_lexicon')` if the resource is not found. I will modify the code to directly download the lexicon if `nltk.data.find` raises a `LookupError`.


"""

import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Download VADER lexicon if not already present
try:
    nltk.data.find('sentiment/vader_lexicon.zip')
except LookupError:
    nltk.download('vader_lexicon')

# Initialize VADER sentiment intensity analyzer
analyzer = SentimentIntensityAnalyzer()

# Get sentiment scores for the cleaned speech text
sentiment_scores = analyzer.polarity_scores(text_clean)

# Print the sentiment scores
print("Sentiment Analysis Results:")
print(sentiment_scores)

# Interpret the results (optional, based on scores)
if sentiment_scores['compound'] > 0.05:
    print("Overall Sentiment: Positive")
elif sentiment_scores['compound'] < -0.05:
    print("Overall Sentiment: Negative")
else:
    print("Overall Sentiment: Neutral")

"""## Summary:

### Data Analysis Key Findings

*   Analyzing the FFT plots for target words indicated that words like "America", "Will", "People", "We", and "together" have a dominant non-zero frequency index of 1, which corresponds to a low relative frequency (0.0068 cycles per sequence). This suggests these words appear somewhat consistently or in relatively large clusters across the speech blocks.
*   A heatmap visualization was successfully generated, showing the frequency of each target word across the speech blocks.
*   Sentiment analysis using NLTK's VADER lexicon on the cleaned speech text resulted in a highly positive compound score of 0.9998, indicating an overall positive sentiment.

### Insights or Next Steps

*   Explore different block sizes for the FFT analysis to understand how the results change and determine if a different granularity reveals more meaningful patterns.
*   Investigate the specific contexts where the identified dominant words appear in clusters to understand the rhetorical strategies or themes associated with their usage.

I will use the `python-docx` library to create the Word document. First, I need to install the library.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install python-docx

"""Now I will generate the Word document."""

from docx import Document
from docx.shared import Inches
import io

# Create a new Document
document = Document()

document.add_heading('Speech Analysis Results', 0)

# Add Sentiment Analysis Results
document.add_heading('Sentiment Analysis Results', level=1)
document.add_paragraph(f"Negative Score: {sentiment_scores['neg']:.4f}")
document.add_paragraph(f"Neutral Score: {sentiment_scores['neu']:.4f}")
document.add_paragraph(f"Positive Score: {sentiment_scores['pos']:.4f}")
document.add_paragraph(f"Compound Score: {sentiment_scores['compound']:.4f}")
if sentiment_scores['compound'] > 0.05:
    document.add_paragraph("Overall Sentiment: Positive")
elif sentiment_scores['compound'] < -0.05:
    document.add_paragraph("Overall Sentiment: Negative")
else:
    document.add_paragraph("Overall Sentiment: Neutral")

# Add Heatmap Visualization
document.add_heading('Word Frequency Heatmap', level=1)

# Save the heatmap to a BytesIO object
img_buffer = io.BytesIO()
plt.figure(figsize=(10, len(targets)*0.8))
sns.heatmap(df_signals.T, cmap="YlGnBu", annot=False, fmt=".0f", linewidths=.5)
plt.xlabel("Block Number")
plt.ylabel("Words")
plt.title("Word Frequency Heatmap Across Speech Blocks")
plt.tight_layout() # Adjust layout to prevent labels overlapping
plt.savefig(img_buffer, format='png')
img_buffer.seek(0)

# Add the image to the document
document.add_picture(img_buffer, width=Inches(6))

# Save the document
document.save('speech_analysis_results.docx')

print("Word document 'speech_analysis_results.docx' created successfully!")

"""# Task
Analyze the frequency and FFT of the most frequent words in the provided speech transcript.

## Find most frequent words

### Subtask:
Determine the most frequently occurring words in the speech.

**Reasoning**:
I need to count the frequency of each word, get the total number of words, identify the most frequent ones, and print the results as instructed. Using `collections.Counter` is an efficient way to count word occurrences.
"""

from collections import Counter

# Count word occurrences
word_counts = Counter(words)

# Get total number of words
total_words = len(words)

# Identify the most frequent words (e.g., top 10)
most_common_words = word_counts.most_common(10)

# Print the results
print(f"Total number of words: {total_words}")
print("\nMost frequent words:")
for word, count in most_common_words:
    print(f"- '{word}': {count}")

"""## Select target words

### Subtask:
Choose a selection of the most frequent words for analysis.

**Reasoning**:
I need to examine the `most_common_words` list and select words for the `targets` list. I will then update the `targets` list with the selected words from the most frequent list. I will select a few from the most common list that haven't been added as targets yet.
"""

# Examine the most_common_words list
print("Most common words:")
for word, count in most_common_words:
    print(f"- '{word}': {count}")

# Select words from the most common list to add to targets
# Avoid adding words already in targets
selected_most_common_targets = [word for word, count in most_common_words if word not in targets]

# Add the selected words to the existing targets list
targets.extend(selected_most_common_targets)

print(f"\nUpdated targets list: {targets}")

"""## Analyze frequency and fft

### Subtask:
Adapt the existing code to generate frequency plots and FFT spectrums for the selected most frequent words.

**Reasoning**:
I need to re-calculate the signals dictionary for the updated list of target words and then regenerate the frequency and FFT plots for all the words in the targets list, adjusting the figure size based on the number of targets.
"""

# Re-calculate signals for all targets
signals = {t: [b.count(t) for b in blocks] for t in targets}

# Re-generate FFT and plots for all words in the updated targets list
fig, axes = plt.subplots(len(targets), 2, figsize=(12, 4*len(targets)))

for row, target in enumerate(targets):
    freq_series = signals[target]
    fft_mag = np.abs(fft(freq_series))
    ax0, ax1 = axes[row]
    ax0.stem(freq_series)
    ax0.set_title(f"Frecuencia de '{target}' por bloque")
    ax0.set_xlabel("Bloque"); ax0.set_ylabel("Cuenta")

    ax1.stem(fft_mag)
    ax1.set_title(f"Espectro FFT de '{target}'")
    ax1.set_xlabel("Frecuencia"); ax1.set_ylabel("Magnitud")

plt.tight_layout()
plt.show()

"""## Interpret results

### Subtask:
Analyze the frequency and FFT results to understand the patterns of usage for the most frequent words.

**Reasoning**:
Examine the generated plots and analyze the FFT spectrum to identify dominant frequencies and discuss their potential meaning in the context of the speech.
"""

# Review the generated plots (visual inspection of the output from the previous cell)
# Identify dominant frequencies (peaks) in each FFT plot, excluding the DC component (frequency 0)
# Discuss what these frequencies might indicate about the usage pattern of the corresponding word

print("Analysis of FFT Plots for Most Frequent Words:")
print("---------------------------------------------")

for target in targets:
    freq_series = signals[target]
    fft_mag = np.abs(fft(freq_series))
    # Exclude DC component for finding dominant non-zero frequencies
    fft_mag_nonzero = fft_mag[1:len(freq_series)//2] # Consider only the positive frequencies

    print(f"\nWord: '{target}'")
    print(f"  Length of signal (number of blocks): {len(freq_series)}")

    if len(fft_mag_nonzero) > 0:
        # Find the index of the maximum magnitude in the non-zero frequency range
        dominant_freq_index_relative = np.argmax(fft_mag_nonzero)
        dominant_freq_index_original = dominant_freq_index_relative + 1 # Add 1 to get the correct index in the original FFT array
        dominant_freq_magnitude = fft_mag[dominant_freq_index_original]

        # The actual frequency value is (dominant_freq_index / len(freq_series)) * sampling_frequency
        # Here, the 'sampling frequency' is 1 block per block, so it's 1.
        # The frequency represents cycles per block sequence.
        relative_dominant_freq = dominant_freq_index_original / len(freq_series)

        print(f"  Dominant non-zero frequency index (in positive spectrum): {dominant_freq_index_relative}")
        print(f"  Corresponding index in full FFT: {dominant_freq_index_original}")
        print(f"  Magnitude at dominant non-zero frequency: {dominant_freq_magnitude:.2f}")
        print(f"  Relative dominant frequency (cycles per sequence): {relative_dominant_freq:.4f}")

        # Discuss potential meaning
        if relative_dominant_freq < 0.05:
            print("  Interpretation: Very low dominant frequency suggests the word appears quite consistently or in very large clusters across the speech.")
        elif relative_dominant_freq < 0.15:
             print("  Interpretation: Low dominant frequency suggests the word appears somewhat consistently or in relatively large clusters.")
        elif relative_dominant_freq < 0.3:
             print("  Interpretation: Medium-low dominant frequency suggests the word appears with some regularity or in moderate clusters.")
        else:
            print("  Interpretation: Higher dominant frequency suggests the word appears more sporadically or in smaller, more dispersed clusters.")

        # Relate to original frequency plot (visual comparison)
        print(f"  Consider the '{target}' frequency plot to see if the pattern aligns visually with this frequency.")
    else:
        print(f"  No significant non-zero frequencies detected.")

print("\nGeneral Interpretation Considerations:")
print("------------------------------------")
print("- A large DC component (magnitude at frequency 0) indicates a high average frequency of the word across the entire speech.")
print("- Dominant non-zero frequencies indicate patterns of repetition or clustering at a certain 'rate' relative to the blocks.")
print("- Keep in mind that FFT assumes periodicity, which word usage in a speech is not strictly.")
print("- The block size significantly influences the frequency analysis.")

"""## Summary:

### Data Analysis Key Findings

*   The total number of words in the transcript is 1463.
*   The most frequent words include 'the' (128), 'and' (73), 'to' (64), 'of' (62), and 'a' (54).
*   For words like 'America', 'Will', 'People', 'We', and 'together', the dominant non-zero frequency in the FFT was very low (relative frequency $\sim$0.0068), suggesting consistent or large clustered usage.
*   Words such as 'country' and 'nation' exhibited medium-low dominant frequencies (relative frequencies $\sim$0.2162 and $\sim$0.1689 respectively), implying usage with some regularity or in moderate clusters.
*   The word 'great' had a higher dominant frequency (relative frequency $\sim$0.4324), indicating potentially more sporadic or smaller, dispersed clusters of usage.

### Insights or Next Steps

*   The FFT analysis provides a quantitative way to understand the rhythm and clustering of word appearances within the speech.
*   Further analysis could involve comparing the FFT patterns of different word categories (e.g., nouns vs. verbs, positive vs. negative words) to see if their usage patterns differ.

I will generate a new Word document with the analysis of the most frequent words.
"""

from docx import Document
from docx.shared import Inches
import io
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Create a new Document
document = Document()

document.add_heading('Most Frequent Word Analysis Results', 0)

# Add Most Frequent Words
document.add_heading('Most Frequent Words', level=1)
document.add_paragraph(f"Total number of words: {total_words}")
document.add_paragraph("Most frequent words:")
for word, count in most_common_words:
    document.add_paragraph(f"- '{word}': {count}")

# Add Frequency Plots and FFTs for Target Words
document.add_heading('Frequency Plots and FFT Spectrums', level=1)

# Re-calculate signals for all targets (if needed, but should be available from previous steps)
# signals = {t: [b.count(t) for b in blocks] for t in targets}

for target in targets:
    document.add_heading(f"Analysis for '{target}'", level=2)

    # Add Frequency Plot
    freq_series = signals[target]
    plt.figure(figsize=(10, 4))
    plt.stem(freq_series)
    plt.title(f"Frecuencia de '{target}' por bloque")
    plt.xlabel("Bloque"); plt.ylabel("Cuenta")
    plt.tight_layout()

    img_buffer_freq = io.BytesIO()
    plt.savefig(img_buffer_freq, format='png')
    img_buffer_freq.seek(0)
    document.add_picture(img_buffer_freq, width=Inches(5))
    plt.close() # Close the figure to free memory

    # Add FFT Spectrum Plot
    fft_mag = np.abs(fft(freq_series))
    plt.figure(figsize=(10, 4))
    plt.stem(fft_mag)
    plt.title(f"Espectro FFT de '{target}'")
    plt.xlabel("Frecuencia"); plt.ylabel("Magnitud")
    plt.tight_layout()

    img_buffer_fft = io.BytesIO()
    plt.savefig(img_buffer_fft, format='png')
    img_buffer_fft.seek(0)
    document.add_picture(img_buffer_fft, width=Inches(5))
    plt.close() # Close the figure to free memory

# Save the document
document.save('most_frequent_word_analysis.docx')

print("Word document 'most_frequent_word_analysis.docx' created successfully!")